{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# huggingface tokenizer를 가져다 사용하기\n",
    "# following the original documentation(https://huggingface.co/docs/tokenizers/quicktour)\n",
    "# tokenizer에 대한 기초적 설명 (https://huggingface.co/learn/nlp-course/ko/chapter2/4)\n",
    "\n",
    "# 환경 구성\n",
    "# install tokenizers first\n",
    "# pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b976024e8f44be8f78ac11818dfbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pretrained tokenizer를 이용함\n",
    "# option : huggingface hub(online)에서 불러와서 사용하기임\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\") # repository name (repository에 토크나이저 모델(json) 있으면 불러와짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-14 03:23:29--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.54.104, 52.217.133.80, 16.182.64.176, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.54.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‘bert-base-uncased-vocab.txt’\n",
      "\n",
      "bert-base-uncased-v 100%[===================>] 226.08K   383KB/s    in 0.6s    \n",
      "\n",
      "2024-05-14 03:23:30 (383 KB/s) - ‘bert-base-uncased-vocab.txt’ saved [231508/231508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# option 2 : vocabulary file을 직접 불러오기임\n",
    "# Classic pretrained BERT tokenizer를 불러옴(해당 토크나이저의 legacy vocabulary file 필요, txt)\n",
    "! wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'my', 'favorite', 'fruit', 'is', 'orange', '.', '[SEP]']\n",
      "[101, 2026, 5440, 5909, 2003, 4589, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# pretrained tokenizer를 사용하여 raw text를 토큰화함 (다큐멘테이션에서 이 결과를 encoding 결과로 지칭) \n",
    "text_case_1 = \"my favorite fruit is orange.\" # English\n",
    "output = tokenizer.encode(text_case_1)\n",
    "# 인코딩 결과를 살펴보면 string을 tokenize해 token list화했음\n",
    "print(output.tokens)\n",
    "# 토큰 순서별로 vocabulary 사전에서의 인덱스도 확인 가능함\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ᄌ', '##ᅦ', '##ᄀ', '##ᅡ', 'ᄌ', '##ᅦ', '##ᄋ', '##ᅵ', '##ᆯ', '[UNK]', 'ᄀ', '##ᅪ', '##ᄋ', '##ᅵ', '##ᆯ', '##ᄋ', '##ᅳ', '##ᆫ', 'ᄋ', '##ᅩ', '##ᄅ', '##ᅦ', '##ᆫ', '##ᄌ', '##ᅵ', '##ᄋ', '##ᅵ', '##ᆸ', '##ᄂ', '##ᅵ', '##ᄃ', '##ᅡ', '.', '[SEP]']\n",
      "\n",
      "[101, 1464, 30009, 29991, 30006, 1464, 30009, 29999, 30019, 30022, 100, 1455, 30012, 29999, 30019, 30022, 29999, 30017, 30021, 1463, 30011, 29994, 30009, 30021, 30000, 30019, 29999, 30019, 30024, 29992, 30019, 29993, 30006, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text_case_2 = \"제가 제일 좋아하는 과일은 오렌지입니다.\" # korean\n",
    "output = tokenizer.encode(text_case_2)\n",
    "print(output.tokens)\n",
    "print() \n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  제가 제일 좋아하는 과일은 오렌지입니다.\n",
      "토큰화 결과:\n",
      " ['[CLS]', 'ᄌ', '##ᅦ', '##ᄀ', '##ᅡ', 'ᄌ', '##ᅦ', '##ᄋ', '##ᅵ', '##ᆯ', '[UNK]', 'ᄀ', '##ᅪ', '##ᄋ', '##ᅵ', '##ᆯ', '##ᄋ', '##ᅳ', '##ᆫ', 'ᄋ', '##ᅩ', '##ᄅ', '##ᅦ', '##ᆫ', '##ᄌ', '##ᅵ', '##ᄋ', '##ᅵ', '##ᆸ', '##ᄂ', '##ᅵ', '##ᄃ', '##ᅡ', '.', '[SEP]']\n",
      "입력 스트링에서의 인덱스 위치: (6, 10)\n",
      "원본 입력에서 관심 토큰에 해당하는 부분:  좋아하는\n"
     ]
    }
   ],
   "source": [
    "# 결과 토큰 조사를 위해 원래 string에서 어떤 부분에서 왔는지 확인해 볼 수 있음\n",
    "# encoding 결과 오브젝트(output)에 원래 string에서의 시작과 끝 위치 정보 형태로 저장되어 있음\n",
    "# (offset method 사용해 반환 가능)\n",
    "\n",
    "print(\"original text: \", text_case_2)\n",
    "print(\"토큰화 결과:\\n\", output.tokens)\n",
    "\n",
    "# encoding 결과의 10번째 위치에 [UNK]' 토큰이 들어있는데, 토크나이즈 전 무엇이었는지 확인\n",
    "# offset 메서드를 이용, 토큰의 원 입력 스트링에서의 위치 정보 반환(start는 토큰에 포함, end는 미포함)\n",
    "print(\"입력 스트링에서의 인덱스 위치:\", output.offsets[10]) \n",
    "print(\"원본 입력에서 관심 토큰에 해당하는 부분: \", text_case_2[output.offsets[10][0]:output.offsets[10][1]]) # 원본 입력을 슬라이싱하여 확인할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "0\n",
      "102\n",
      "[CLS]\n",
      "[PAD]\n",
      "[SEP]\n",
      "None\n",
      "None\n",
      "None\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "# 한편, tokenizer instance의 token_to_id는 개별 토큰이 직접 vocabulary의 몇 번 인덱스에 해당하는지 조사 가능\n",
    "print(tokenizer.token_to_id(\"[CLS]\"))\n",
    "print(tokenizer.token_to_id(\"[PAD]\"))\n",
    "print(tokenizer.token_to_id(\"[SEP]\"))\n",
    "\n",
    "print(tokenizer.id_to_token(101))\n",
    "print(tokenizer.id_to_token(0))\n",
    "print(tokenizer.id_to_token(102))\n",
    "\n",
    "print(tokenizer.token_to_id(\"ㅈ\"))\n",
    "print(tokenizer.token_to_id(\"##ㅔ\"))\n",
    "print(tokenizer.token_to_id(\"##ㄱ\"))\n",
    "\n",
    "print(tokenizer.get_vocab_size())\n",
    "\n",
    "# tokenizer api doc: https://huggingface.co/docs/tokenizers/api/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 한편, 토큰화된 결과를 직접 모델에 입력하기 전에 원하는 위치에 구분자 등의 스페셜 토큰을 삽입하는 후처리를 할 수 있음\n",
    "# 템플릿 없이 토큰화한 결과\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
    "print(output.tokens)\n",
    "# [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 방법 커스텀해 토큰화한 결과\n",
    "# TemplateProcessing을 활용한 토큰화 후처리 \"템플릿\" 정의함\n",
    "\"\"\"\n",
    "아래 템플릿은\n",
    "encoder 메서드가 단일, 또는 쌍으로 된 스트링 입력을 받아 단일 encoding object로 토큰화할 때의 후처리 템플릿을 정의함\n",
    "- single 인자 입력은 [CLS] [SEP] 사이에 입력한 문장을 넣어 후처리할 것임을 의미함($A가 문장을 나타냄)\n",
    "- pair 인자 입력은 [CLS] [SEP] [SEP] 사이에 두 문장이 들어오게 후처리할 것임을 의미함($A와 $B가 각 문장을 나타냄)\n",
    "    - pair 인자 입력에서 $B뒤에 붙은 :1 은 해당 입력의 각 부분에 1번 번호를 주길 원한다는 뜻임\n",
    "      (아무 지정없을 때 기본은 0번, 이 번호는 type IDs로 지칭되며 type_ids 메서드로 호출 가능함)\n",
    "- special_token 인자 입력은 이용된 special token과 \n",
    "  special 토큰의 우리가 사용한 tokenizer의 vocabulary 사전에서의 룩업 인덱스(tokenizer의 token_to_id로 확인 가능)\n",
    "\"\"\"\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 문장들을 인코딩할 수 있음\n",
    "output = tokenizer.encode(\"Hello, y'all!\", \"How are you 😁 ?\")\n",
    "print(output.tokens)\n",
    "# [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"[SEP]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n",
    "\n",
    "print(output.type_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ᄌ', '##ᅦ', '##ᄀ', '##ᅡ', 'ᄌ', '##ᅦ', '##ᄋ', '##ᅵ', '##ᆯ', '[UNK]', 'ᄀ', '##ᅪ', '##ᄋ', '##ᅵ', '##ᆯ', '##ᄋ', '##ᅳ', '##ᆫ', 'ᄋ', '##ᅩ', '##ᄅ', '##ᅦ', '##ᆫ', '##ᄌ', '##ᅵ', '##ᄋ', '##ᅵ', '##ᆸ', '##ᄂ', '##ᅵ', '##ᄃ', '##ᅡ', '.', '[SEP]', 'ᄋ', '##ᅩ', '##ᄅ', '##ᅦ', '##ᆫ', '##ᄌ', '##ᅵ', '##ᄅ', '##ᅳ', '##ᆯ', '[UNK]', 'ᄉ', '##ᅡ', '##ᄅ', '##ᅡ', '##ᆷ', '##ᄋ', '##ᅳ', '##ᆫ', '[UNK]', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"제가 제일 좋아하는 과일은 오렌지입니다.\", \"오렌지를 좋아하는 사람은 없습니다.\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 단위 encoding은 Tokenizer를 빠르게 사용할 수 있게 해줌(Tokenizer.encode_batch)\n",
    "# output은 여러 encoding object들의 리스트임\n",
    "batch_text = [\"제가 제일 좋아하는 과일은 오렌지입니다.\", \"오렌지를 좋아하는 사람은 없습니다.\"]\n",
    "output = tokenizer.encode_batch(batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쌍으로 된 입력을 처리해야 하는 경우 두 개의 리스트를 encode_batch 메서드에 입력하도록 함\n",
    "# sentences A의 리스트, sentences B의 리스트\n",
    "output = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you 😁 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "['[CLS]', 'hello', 'to', 'you', 'too', '!', '[SEP]', 'i', \"'\", 'm', 'fine', ',', 'thank', 'you', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you 😁 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"], [\"is this the real life?\", \"is this just fantasy?\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "['[CLS]', 'hello', 'to', 'you', 'too', '!', '[SEP]', 'i', \"'\", 'm', 'fine', ',', 'thank', 'you', '!', '[SEP]']\n",
      "['[CLS]', 'is', 'this', 'the', 'real', 'life', '?', '[SEP]', 'is', 'this', 'just', 'fantasy', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(output[0].tokens)\n",
    "print(output[1].tokens)\n",
    "print(output[2].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "101\n",
      "[unused2]\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 배치를 처리하기에 앞서 모두 같은 길이로 패딩되게 처리할 수도 있음\n",
    "# pad token의 vocab id\n",
    "print(tokenizer.token_to_id(\"[PAD]\"))\n",
    "print(tokenizer.token_to_id(\"[CLS]\"))\n",
    "print(tokenizer.id_to_token(3))\n",
    "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# huggingface tokenizerë¥¼ ê°€ì ¸ë‹¤ ì‚¬ìš©í•˜ê¸°\n",
    "# following the original documentation(https://huggingface.co/docs/tokenizers/quicktour)\n",
    "# tokenizerì— ëŒ€í•œ ê¸°ì´ˆì  ì„¤ëª… (https://huggingface.co/learn/nlp-course/ko/chapter2/4)\n",
    "\n",
    "# í™˜ê²½ êµ¬ì„±\n",
    "# install tokenizers first\n",
    "# pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b976024e8f44be8f78ac11818dfbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pretrained tokenizerë¥¼ ì´ìš©í•¨\n",
    "# option : huggingface hub(online)ì—ì„œ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•˜ê¸°ì„\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\") # repository name (repositoryì— í† í¬ë‚˜ì´ì € ëª¨ë¸(json) ìˆìœ¼ë©´ ë¶ˆëŸ¬ì™€ì§)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-14 03:23:29--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.54.104, 52.217.133.80, 16.182.64.176, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.54.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: â€˜bert-base-uncased-vocab.txtâ€™\n",
      "\n",
      "bert-base-uncased-v 100%[===================>] 226.08K   383KB/s    in 0.6s    \n",
      "\n",
      "2024-05-14 03:23:30 (383 KB/s) - â€˜bert-base-uncased-vocab.txtâ€™ saved [231508/231508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# option 2 : vocabulary fileì„ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ê¸°ì„\n",
    "# Classic pretrained BERT tokenizerë¥¼ ë¶ˆëŸ¬ì˜´(í•´ë‹¹ í† í¬ë‚˜ì´ì €ì˜ legacy vocabulary file í•„ìš”, txt)\n",
    "! wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'my', 'favorite', 'fruit', 'is', 'orange', '.', '[SEP]']\n",
      "[101, 2026, 5440, 5909, 2003, 4589, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# pretrained tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ raw textë¥¼ í† í°í™”í•¨ (ë‹¤íë©˜í…Œì´ì…˜ì—ì„œ ì´ ê²°ê³¼ë¥¼ encoding ê²°ê³¼ë¡œ ì§€ì¹­) \n",
    "text_case_1 = \"my favorite fruit is orange.\" # English\n",
    "output = tokenizer.encode(text_case_1)\n",
    "# ì¸ì½”ë”© ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´ stringì„ tokenizeí•´ token listí™”í–ˆìŒ\n",
    "print(output.tokens)\n",
    "# í† í° ìˆœì„œë³„ë¡œ vocabulary ì‚¬ì „ì—ì„œì˜ ì¸ë±ìŠ¤ë„ í™•ì¸ ê°€ëŠ¥í•¨\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'á„Œ', '##á…¦', '##á„€', '##á…¡', 'á„Œ', '##á…¦', '##á„‹', '##á…µ', '##á†¯', '[UNK]', 'á„€', '##á…ª', '##á„‹', '##á…µ', '##á†¯', '##á„‹', '##á…³', '##á†«', 'á„‹', '##á…©', '##á„…', '##á…¦', '##á†«', '##á„Œ', '##á…µ', '##á„‹', '##á…µ', '##á†¸', '##á„‚', '##á…µ', '##á„ƒ', '##á…¡', '.', '[SEP]']\n",
      "\n",
      "[101, 1464, 30009, 29991, 30006, 1464, 30009, 29999, 30019, 30022, 100, 1455, 30012, 29999, 30019, 30022, 29999, 30017, 30021, 1463, 30011, 29994, 30009, 30021, 30000, 30019, 29999, 30019, 30024, 29992, 30019, 29993, 30006, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text_case_2 = \"ì œê°€ ì œì¼ ì¢‹ì•„í•˜ëŠ” ê³¼ì¼ì€ ì˜¤ë Œì§€ì…ë‹ˆë‹¤.\" # korean\n",
    "output = tokenizer.encode(text_case_2)\n",
    "print(output.tokens)\n",
    "print() \n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  ì œê°€ ì œì¼ ì¢‹ì•„í•˜ëŠ” ê³¼ì¼ì€ ì˜¤ë Œì§€ì…ë‹ˆë‹¤.\n",
      "í† í°í™” ê²°ê³¼:\n",
      " ['[CLS]', 'á„Œ', '##á…¦', '##á„€', '##á…¡', 'á„Œ', '##á…¦', '##á„‹', '##á…µ', '##á†¯', '[UNK]', 'á„€', '##á…ª', '##á„‹', '##á…µ', '##á†¯', '##á„‹', '##á…³', '##á†«', 'á„‹', '##á…©', '##á„…', '##á…¦', '##á†«', '##á„Œ', '##á…µ', '##á„‹', '##á…µ', '##á†¸', '##á„‚', '##á…µ', '##á„ƒ', '##á…¡', '.', '[SEP]']\n",
      "ì…ë ¥ ìŠ¤íŠ¸ë§ì—ì„œì˜ ì¸ë±ìŠ¤ ìœ„ì¹˜: (6, 10)\n",
      "ì›ë³¸ ì…ë ¥ì—ì„œ ê´€ì‹¬ í† í°ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„:  ì¢‹ì•„í•˜ëŠ”\n"
     ]
    }
   ],
   "source": [
    "# ê²°ê³¼ í† í° ì¡°ì‚¬ë¥¼ ìœ„í•´ ì›ë˜ stringì—ì„œ ì–´ë–¤ ë¶€ë¶„ì—ì„œ ì™”ëŠ”ì§€ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŒ\n",
    "# encoding ê²°ê³¼ ì˜¤ë¸Œì íŠ¸(output)ì— ì›ë˜ stringì—ì„œì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ ì •ë³´ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ\n",
    "# (offset method ì‚¬ìš©í•´ ë°˜í™˜ ê°€ëŠ¥)\n",
    "\n",
    "print(\"original text: \", text_case_2)\n",
    "print(\"í† í°í™” ê²°ê³¼:\\n\", output.tokens)\n",
    "\n",
    "# encoding ê²°ê³¼ì˜ 10ë²ˆì§¸ ìœ„ì¹˜ì— [UNK]' í† í°ì´ ë“¤ì–´ìˆëŠ”ë°, í† í¬ë‚˜ì´ì¦ˆ ì „ ë¬´ì—‡ì´ì—ˆëŠ”ì§€ í™•ì¸\n",
    "# offset ë©”ì„œë“œë¥¼ ì´ìš©, í† í°ì˜ ì› ì…ë ¥ ìŠ¤íŠ¸ë§ì—ì„œì˜ ìœ„ì¹˜ ì •ë³´ ë°˜í™˜(startëŠ” í† í°ì— í¬í•¨, endëŠ” ë¯¸í¬í•¨)\n",
    "print(\"ì…ë ¥ ìŠ¤íŠ¸ë§ì—ì„œì˜ ì¸ë±ìŠ¤ ìœ„ì¹˜:\", output.offsets[10]) \n",
    "print(\"ì›ë³¸ ì…ë ¥ì—ì„œ ê´€ì‹¬ í† í°ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„: \", text_case_2[output.offsets[10][0]:output.offsets[10][1]]) # ì›ë³¸ ì…ë ¥ì„ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ í™•ì¸í•  ìˆ˜ ìˆìŒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "0\n",
      "102\n",
      "[CLS]\n",
      "[PAD]\n",
      "[SEP]\n",
      "None\n",
      "None\n",
      "None\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "# í•œí¸, tokenizer instanceì˜ token_to_idëŠ” ê°œë³„ í† í°ì´ ì§ì ‘ vocabularyì˜ ëª‡ ë²ˆ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ”ì§€ ì¡°ì‚¬ ê°€ëŠ¥\n",
    "print(tokenizer.token_to_id(\"[CLS]\"))\n",
    "print(tokenizer.token_to_id(\"[PAD]\"))\n",
    "print(tokenizer.token_to_id(\"[SEP]\"))\n",
    "\n",
    "print(tokenizer.id_to_token(101))\n",
    "print(tokenizer.id_to_token(0))\n",
    "print(tokenizer.id_to_token(102))\n",
    "\n",
    "print(tokenizer.token_to_id(\"ã…ˆ\"))\n",
    "print(tokenizer.token_to_id(\"##ã…”\"))\n",
    "print(tokenizer.token_to_id(\"##ã„±\"))\n",
    "\n",
    "print(tokenizer.get_vocab_size())\n",
    "\n",
    "# tokenizer api doc: https://huggingface.co/docs/tokenizers/api/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# í•œí¸, í† í°í™”ëœ ê²°ê³¼ë¥¼ ì§ì ‘ ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ì „ì— ì›í•˜ëŠ” ìœ„ì¹˜ì— êµ¬ë¶„ì ë“±ì˜ ìŠ¤í˜ì…œ í† í°ì„ ì‚½ì…í•˜ëŠ” í›„ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŒ\n",
    "# í…œí”Œë¦¿ ì—†ì´ í† í°í™”í•œ ê²°ê³¼\n",
    "output = tokenizer.encode(\"Hello, y'all! How are you ğŸ˜ ?\")\n",
    "print(output.tokens)\n",
    "# [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›„ì²˜ë¦¬ ë°©ë²• ì»¤ìŠ¤í…€í•´ í† í°í™”í•œ ê²°ê³¼\n",
    "# TemplateProcessingì„ í™œìš©í•œ í† í°í™” í›„ì²˜ë¦¬ \"í…œí”Œë¦¿\" ì •ì˜í•¨\n",
    "\"\"\"\n",
    "ì•„ë˜ í…œí”Œë¦¿ì€\n",
    "encoder ë©”ì„œë“œê°€ ë‹¨ì¼, ë˜ëŠ” ìŒìœ¼ë¡œ ëœ ìŠ¤íŠ¸ë§ ì…ë ¥ì„ ë°›ì•„ ë‹¨ì¼ encoding objectë¡œ í† í°í™”í•  ë•Œì˜ í›„ì²˜ë¦¬ í…œí”Œë¦¿ì„ ì •ì˜í•¨\n",
    "- single ì¸ì ì…ë ¥ì€ [CLS] [SEP] ì‚¬ì´ì— ì…ë ¥í•œ ë¬¸ì¥ì„ ë„£ì–´ í›„ì²˜ë¦¬í•  ê²ƒì„ì„ ì˜ë¯¸í•¨($Aê°€ ë¬¸ì¥ì„ ë‚˜íƒ€ëƒ„)\n",
    "- pair ì¸ì ì…ë ¥ì€ [CLS] [SEP] [SEP] ì‚¬ì´ì— ë‘ ë¬¸ì¥ì´ ë“¤ì–´ì˜¤ê²Œ í›„ì²˜ë¦¬í•  ê²ƒì„ì„ ì˜ë¯¸í•¨($Aì™€ $Bê°€ ê° ë¬¸ì¥ì„ ë‚˜íƒ€ëƒ„)\n",
    "    - pair ì¸ì ì…ë ¥ì—ì„œ $Bë’¤ì— ë¶™ì€ :1 ì€ í•´ë‹¹ ì…ë ¥ì˜ ê° ë¶€ë¶„ì— 1ë²ˆ ë²ˆí˜¸ë¥¼ ì£¼ê¸¸ ì›í•œë‹¤ëŠ” ëœ»ì„\n",
    "      (ì•„ë¬´ ì§€ì •ì—†ì„ ë•Œ ê¸°ë³¸ì€ 0ë²ˆ, ì´ ë²ˆí˜¸ëŠ” type IDsë¡œ ì§€ì¹­ë˜ë©° type_ids ë©”ì„œë“œë¡œ í˜¸ì¶œ ê°€ëŠ¥í•¨)\n",
    "- special_token ì¸ì ì…ë ¥ì€ ì´ìš©ëœ special tokenê³¼ \n",
    "  special í† í°ì˜ ìš°ë¦¬ê°€ ì‚¬ìš©í•œ tokenizerì˜ vocabulary ì‚¬ì „ì—ì„œì˜ ë£©ì—… ì¸ë±ìŠ¤(tokenizerì˜ token_to_idë¡œ í™•ì¸ ê°€ëŠ¥)\n",
    "\"\"\"\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ë“¤ì„ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŒ\n",
    "output = tokenizer.encode(\"Hello, y'all!\", \"How are you ğŸ˜ ?\")\n",
    "print(output.tokens)\n",
    "# [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"[SEP]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]\n",
    "\n",
    "print(output.type_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'á„Œ', '##á…¦', '##á„€', '##á…¡', 'á„Œ', '##á…¦', '##á„‹', '##á…µ', '##á†¯', '[UNK]', 'á„€', '##á…ª', '##á„‹', '##á…µ', '##á†¯', '##á„‹', '##á…³', '##á†«', 'á„‹', '##á…©', '##á„…', '##á…¦', '##á†«', '##á„Œ', '##á…µ', '##á„‹', '##á…µ', '##á†¸', '##á„‚', '##á…µ', '##á„ƒ', '##á…¡', '.', '[SEP]', 'á„‹', '##á…©', '##á„…', '##á…¦', '##á†«', '##á„Œ', '##á…µ', '##á„…', '##á…³', '##á†¯', '[UNK]', 'á„‰', '##á…¡', '##á„…', '##á…¡', '##á†·', '##á„‹', '##á…³', '##á†«', '[UNK]', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"ì œê°€ ì œì¼ ì¢‹ì•„í•˜ëŠ” ê³¼ì¼ì€ ì˜¤ë Œì§€ì…ë‹ˆë‹¤.\", \"ì˜¤ë Œì§€ë¥¼ ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒì€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch ë‹¨ìœ„ encodingì€ Tokenizerë¥¼ ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ(Tokenizer.encode_batch)\n",
    "# outputì€ ì—¬ëŸ¬ encoding objectë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì„\n",
    "batch_text = [\"ì œê°€ ì œì¼ ì¢‹ì•„í•˜ëŠ” ê³¼ì¼ì€ ì˜¤ë Œì§€ì…ë‹ˆë‹¤.\", \"ì˜¤ë Œì§€ë¥¼ ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒì€ ì—†ìŠµë‹ˆë‹¤.\"]\n",
    "output = tokenizer.encode_batch(batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŒìœ¼ë¡œ ëœ ì…ë ¥ì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ê²½ìš° ë‘ ê°œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ encode_batch ë©”ì„œë“œì— ì…ë ¥í•˜ë„ë¡ í•¨\n",
    "# sentences Aì˜ ë¦¬ìŠ¤íŠ¸, sentences Bì˜ ë¦¬ìŠ¤íŠ¸\n",
    "output = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you ğŸ˜ ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "['[CLS]', 'hello', 'to', 'you', 'too', '!', '[SEP]', 'i', \"'\", 'm', 'fine', ',', 'thank', 'you', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you ğŸ˜ ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"], [\"is this the real life?\", \"is this just fantasy?\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n",
      "['[CLS]', 'hello', 'to', 'you', 'too', '!', '[SEP]', 'i', \"'\", 'm', 'fine', ',', 'thank', 'you', '!', '[SEP]']\n",
      "['[CLS]', 'is', 'this', 'the', 'real', 'life', '?', '[SEP]', 'is', 'this', 'just', 'fantasy', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(output[0].tokens)\n",
    "print(output[1].tokens)\n",
    "print(output[2].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "101\n",
      "[unused2]\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ ê°œì˜ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê¸°ì— ì•ì„œ ëª¨ë‘ ê°™ì€ ê¸¸ì´ë¡œ íŒ¨ë”©ë˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ë„ ìˆìŒ\n",
    "# pad tokenì˜ vocab id\n",
    "print(tokenizer.token_to_id(\"[PAD]\"))\n",
    "print(tokenizer.token_to_id(\"[CLS]\"))\n",
    "print(tokenizer.id_to_token(3))\n",
    "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
